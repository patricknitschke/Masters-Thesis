\chapter{Acronyms}
\label{app:acronyms}

\textbf{Q} - Action-value \\[-4mm]

\noindent
\textbf{DQNs} - Deep Q-Networks \\[-4mm]

\noindent
\textbf{DPG} - Deterministic Policy Gradients \\[-4mm]

\noindent
\textbf{DDPG} - Deep Deterministic Policy Gradients \\[-4mm]

\noindent
\textbf{TRPO} - Trust-Region Policy Optimisation \\[-4mm]

\noindent
\textbf{ACER} - Actor-Critic Experience Replay \\[-4mm]

\noindent
\textbf{PPO} - Proximal Policy Optimization \\[-4mm]

\noindent
\textbf{MDPs} - Finite Markov Decision Processes \\[-4mm]

\noindent
\textbf{POMDPs} - Partially Observed Markov Decision Processes \\[-4mm]

\noindent
\textbf{TD} - Temporal difference \\[-4mm]

\noindent
$\boldsymbol{\overline{VE}}$ - Prediction error \\[-4mm]

\noindent
\textbf{SARSA} - State-Action-Reward-State-Action \\[-4mm]

\noindent
\textbf{NNs} - Neural networks \\[-4mm]

\noindent
\textbf{DNNs} - Deep neural networks \\[-4mm]

\noindent
\textbf{CNNs} - Convolutional neural networks \\[-4mm]

\noindent
\textbf{RNNs} - Recurrent neural networks \\[-4mm]

\noindent
\textbf{MLP} - Multi-layer Perceptron \\[-4mm]

\noindent
\textbf{KL} - Kullback-Leibler \\[-4mm]

\noindent
\textbf{MSE} - Mean-squared error \\[-4mm]

\noindent
\textbf{RMSE} - Root-mean-squared error \\[-4mm]

\noindent
\textbf{QP} - Quadratic Programming \\[-4mm]

\noindent
\textbf{PD controller} - Proportional-derivative controller \\[-4mm]

\noindent
\textbf{RMF} - Resilient micro-flyer \\[-4mm]

\noindent
\textbf{MAV} - Micro aerial vehicle \\[-4mm]

\noindent
\textbf{URDF} -  Unified Robotic Description Format \\[-4mm]

\noindent
\textbf{ROS} - Robot Operating System \\[-4mm]



