\chapter{Introduction}
\label{chap:1_introduction}

\section{Motivation}
Autonomous navigation of robotic vehicles is a complicated task that has challenged the cybernetics community since its inception.
It is a research field of substantial focus particularly in recent years, given its novel applications within commercial sectors \cite{eelumeSale, anymalInTheField, CogniteVisualInspection}, transportation \cite{autoferryNTNU}, search and rescue \cite{washingtonPost}, and defence \cite{darpaAutonomy}. As new possibilities for autonomy are increasing, so is the need to find new, innovative, robust and safe solutions to meet this demand.

The difficulty of autonomous navigation in cluttered and dynamic environments arises from the combination of three separate tasks: first to perceive the local environment directly from on-board sensors, then to predict how the environment will evolve, and finally to decide on a safe and intelligent action based on the inferred information \cite{darpaCar}. Each of these tasks presents its challenges, such as dealing with noise or uncertainty in sensor data or feasible trajectory planning, but separating these tasks into a threefold process ultimately leads to an increased latency and compounding of errors in the pipeline \cite{HighSpeedFlightWild}.

Moreover, as robotic use-cases are becoming more advanced -- such as in underwater \cite{eelume}, forested \cite{HighSpeedFlightWild} and subterranean environments \cite{GBplanner} -- autonomous robots now have to contend with environments that are: sensor-degraded with limited illumination; long, narrow and multi-branching; unpredictable and unstructured; isolated from external communications. Though localisation and mapping techniques based on 3D perception have been successful in unstructured environments in the past \cite{navigationSLAM}, the characteristics of these newer domains present new challenges for traditional approaches. Specifically, these environments make it difficult to maintain an accurate map of the environment, puts the tractability of trajectory planning into question and limits the amount of computational resources available to the robot \cite{LearningStateRepresentation, NavRep_unsupervised, HighSpeedFlightWild, LBplanner}.

Not to mention, navigation within cluttered environments requires fast, accurate and careful planning of versatile vehicles -- such as in multirotor aerial vehicles (quadrotors) \cite{MultirotorAerialVehicles}. This requires a quick mapping from sensor data to action, which makes a high-latency solution non-viable because of the inherent difficulty of pose estimation when travelling at high speeds \cite{NavRep_unsupervised, HighSpeedFlightWild}. 

Therefore, these issues prompt the consideration of learning-based methods to directly infer actions from raw sensor input, as an alternative to the three-subtask, model-based pipeline. The idea is to remove the necessity for accurate maps, though retaining essential features, and using this to plan feasible trajectories even in complex edge-case scenarios.
Utilising a data-driven approach should allow an agent to capture the system's dynamics and the environment's uncertainties without the need for any explicit programming \cite{RLinRoboticsSurvey} -- thus removing the need for feature engineering or heuristics to make the navigation task tractable \cite{LearningStateRepresentation}. 
The processing time dramatically decreases due to this direct mapping, as sensor data does not need to be preprocessed into higher dimensional information \cite{fan2018distributed, stereoCamTracking}, maps will not have to be generated or queried, and exhaustive collision checking is avoided \cite{LBplanner}.
Instead, a learning approach will be used to extract high-dimensional information directly, capable of filtering out redundant information in LiDAR and depth data. Then, the agent can learn collision avoidance based on experience from these high-level features \cite{LearningStateRepresentation}. 

The apparent limitation of using learning-based methods is that the amount of data required to solve complex tasks is proportional to its complexity \cite{LearningWalkMassivelyParallel}, where varied environments are often also required in the learning process \cite{RichEnvironments}. Due to this, the question of whether or not a task was learnable through reinforcement learning became simply a question of time or computational resources. If these resources were unavailable, more sample-efficient methods had to be explored, for example: supervised learning through clever use of datasets \cite{cad2rl, dronet}, engineering of action spaces and learning these in a self-supervised manner \cite{deepCollisionPredictorOracle}, or by imitation learning using an expert planner \cite{LBplanner, pfeiffer2017perception, HighSpeedFlightWild}.

However, until recently, the research community has been developing parallel end-to-end hardware-accelerated (GPU) simulators, such as \textit{Isaac Gym}, which have provided the opportunity to simulate tens of thousands of environments in parallel and ``enables the solving of tasks with a single GPU that were previously only possible on massive CPU clusters'' \cite{IsaacGym}. This has opened up a multitude of possibilities for autonomous navigation using reinforcement learning, thus being the motivation for this thesis.


\section{Scope}
This thesis explores how a mapless navigation policy for collision avoidance can be learned on a quadrotor without expert demonstrations by leveraging novel ideas in learning-based autonomous navigation combined with a massively parallel learning scheme (Isaac Gym). 
Specifically, the aim is to infer a reference velocity and steering angle for the quadrotor given its state and depth image from a forward-facing depth camera. With this policy, an agent should be able to navigate through a cluttered environment to a goal specified in three-dimensional space.

To learn this policy, this thesis will present the theory and implementation of a two-part deep neural network module. The first module, a variational autoencoder (VAE), is tasked with extracting the essential information from a depth image. The second module will be a fully-connected neural network (or multi-layer perceptron) and serves as the reinforcement learning agent. The agent will receive the essential information of the depth image (i.e. its features), along with the quadrotor state, and decide on a velocity and yaw rate reference for the quadrotor. To optimise the agent, the Proximal Policy Optimisation (PPO) algorithm will be used. 

Additionally, as the VAE and simulation environment is set up from scratch, the design choices for each module will also be discussed in light of intermediate results during the development phase.
For the VAE, this includes the choice of architecture and loss functions for improvements in the reconstructed images.
For the MLP, this includes simulation setup and gradual training steps (curriculum) to achieve a stable training process and eventually collision avoidance.

Finally, the intermediate and final agents will be tested in cluttered environments with varying difficulty, where their success (and failure) will be analysed. 

\todo{example of results,
video and link}


\section{Outline}

The outline of this thesis will be as follows:
\vspace{2mm}
\begin{itemize}
    \item \textbf{Chapter 1: Introduction} \\
    Motivation for this thesis is presented, along with the ... \vspace{3mm}

    \item \textbf{Chapter 2: Theoretical Background}
    \vspace{3mm}
    
    \item \textbf{Chapter 3: Related Works} 
    \vspace{3mm}
    
    \item \textbf{Chapter 4: Problem Formulation} 
    \vspace{3mm}
    
    \item \textbf{Chapter 5: Proposed Approach}
    \vspace{3mm}
    
    \item \textbf{Chapter 6: Implementation} 
    \vspace{3mm}
    
    \item \textbf{Chapter 7: Navigation Policy Evaluation Studies} 
    \vspace{3mm}
    
    \item \textbf{Chapter 8: VAE Evaluation Studies} 
    \vspace{3mm}
    
    % \item \textbf{Chapter 9: Discussion} 
    \vspace{3mm}
    
    \item \textbf{Chapter 9: Conclusions} \vspace{3mm}
    
\end{itemize}





