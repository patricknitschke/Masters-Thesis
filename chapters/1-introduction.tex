\chapter{Introduction}
\label{chap:1_introduction}

\section{Motivation}
Autonomous navigation of robotic vehicles is a complicated task that has challenged the cybernetics community since its inception.
It is a research field of substantial focus particularly in recent years, given its novel applications within commercial sectors \cite{eelumeSale, anymalInTheField, CogniteVisualInspection}, transportation \cite{autoferryNTNU}, search and rescue \cite{washingtonPost}, and defence \cite{darpaAutonomy}. As new possibilities for autonomy are increasing, so is the need to find new, innovative, robust and safe solutions to meet this demand.

The difficulty of autonomous navigation in cluttered and dynamic environments arises from the combination of three separate tasks: first to perceive the local environment directly from on-board sensors, then to predict how the environment will evolve, and finally to decide on a safe and intelligent action based on the inferred information \cite{darpaCar}. Each of these tasks presents its challenges, such as dealing with noise or uncertainty in sensor data or feasible trajectory planning, but separating these tasks into a threefold process ultimately leads to an increased latency and compounding of errors in the pipeline \cite{HighSpeedFlightWild}.

Moreover, as robotic use-cases are becoming more advanced -- such as in underwater \cite{eelume}, forested \cite{HighSpeedFlightWild} and subterranean environments \cite{GBplanner} -- autonomous robots now have to contend with environments that are: sensor-degraded with limited illumination; long, narrow and multi-branching; unpredictable and unstructured; isolated from external communications. Though localisation and mapping techniques based on 3D perception have been successful in unstructured environments in the past \cite{navigationSLAM}, the characteristics of these newer domains present new challenges for traditional approaches. Specifically, these environments make it difficult to maintain an accurate map of the environment, puts the tractability of trajectory planning into question and limits the amount of computational resources available to the robot \cite{LearningStateRepresentation, NavRep_unsupervised, HighSpeedFlightWild, LBplanner}.

Not to mention, navigation within cluttered environments requires fast, accurate and careful planning of versatile vehicles -- such as in multirotor aerial vehicles (quadrotors) \cite{MultirotorAerialVehicles}. This requires a quick mapping from sensor data to action, which makes a high-latency solution non-viable because of the inherent difficulty of pose estimation when travelling at high speeds \cite{NavRep_unsupervised, HighSpeedFlightWild}. 

Therefore, these issues prompt the consideration of learning-based methods to directly infer actions from raw sensor input, as an alternative to the three-subtask, model-based pipeline. The idea is to remove the necessity for accurate maps, though retaining essential features, and using this to plan feasible trajectories even in complex edge-case scenarios.
Utilising a data-driven approach should allow an agent to capture the system's dynamics and the environment's uncertainties without the need for any explicit programming \cite{RLinRoboticsSurvey} -- thus removing the need for feature engineering or heuristics to make the navigation task tractable \cite{LearningStateRepresentation}. 
The processing time dramatically decreases due to this direct mapping, as sensor data does not need to be preprocessed into higher dimensional information \cite{fan2018distributed, stereoCamTracking}, maps will not have to be generated or queried, and exhaustive collision checking is avoided \cite{LBplanner}.
Instead, a learning approach will be used to extract high-dimensional information directly, capable of filtering out redundant information in LiDAR and depth data. Then, the agent can learn collision avoidance based on experience from these high-level features \cite{LearningStateRepresentation}. 

The apparent limitation of using learning-based methods is that the amount of data required to solve complex tasks is proportional to its complexity \cite{LearningWalkMassivelyParallel}, where varied environments are often also required in the learning process \cite{RichEnvironments}. Due to this, the question of whether or not a task was learnable through reinforcement learning became simply a question of time or computational resources. If these resources were unavailable, more sample-efficient methods had to be explored, for example: supervised learning through clever use of datasets \cite{cad2rl, dronet}, engineering of action spaces and learning these in a self-supervised manner \cite{deepCollisionPredictorOracle}, or by imitation learning using an expert planner \cite{LBplanner, pfeiffer2017perception, HighSpeedFlightWild}.

However, until recently, the research community has been developing parallel end-to-end hardware-accelerated (GPU) simulators, such as \textit{Isaac Gym}, which have provided the opportunity to simulate tens of thousands of environments in parallel and ``enables the solving of tasks with a single GPU that were previously only possible on massive CPU clusters'' \cite{IsaacGym}. This has opened up a multitude of possibilities for autonomous navigation using reinforcement learning, thus being the motivation for this thesis.


\section{Scope}
This thesis explores how a mapless navigation policy for collision avoidance can be learned on a quadrotor without expert demonstrations by leveraging novel ideas in learning-based autonomous navigation combined with a massively parallel learning scheme (Isaac Gym). 
Specifically, the aim is to infer a reference velocity and steering angle for the quadrotor given its state and depth image from a forward-facing depth camera. With this policy, an agent should be able to navigate through a cluttered environment to some goal specified in three-dimensional space, such that it can be combined with some global path planner.

To learn this policy, this thesis will present the theory and implementation of a two-part deep neural network module. The first module, a variational autoencoder (VAE), is tasked with extracting the essential information or features from a depth image. The second module will be a fully-connected neural network (or multi-layer perceptron) and serves as the reinforcement learning agent. The agent will receive the essential information of the depth image (i.e. its features), along with the quadrotor state, and decide on a velocity and yaw rate reference for the quadrotor. To optimise the VAE, we use the Auto-Encoding Variational Bayes (AEVB) algorithm, while agent is optimised through Proximal Policy Optimisation (PPO). We also define a custom loss function for the VAE to define important features to be prioritised, and define a reward function that is conditions an agent to be collision avoidant.

Additionally, as the VAE and MLP modules are written from scratch, the design choices and training for each module will be presented in detail. 
For the VAE, this includes the choice of architecture and loss functions for improvements in the reconstructed images.
For the MLP, this includes simulation setup and gradual training steps (curriculum) to achieve a stable training process and eventually collision avoidance.
Finally, this thesis will evaluate the performance of the trained policy in a series of standardised tests, ranging from known environments with varying difficult, robustness tests to noise and finally generalisation to larger domains. \todo{VAE?}

\todo{example of results,
video and link}


\section{Outline}

The outline of this thesis will be as follows:
\vspace{2mm}
\begin{itemize}
    \item \textbf{Chapter 1: Introduction} \\
    The motivation for this thesis is presented, along with the scope of the task and the content of this thesis. \vspace{3mm}

    \item \textbf{Chapter 2: Theoretical Background}
    The underlying theory for VAE is presented through the lens of unsupervised representation learning, where fundamental deep learning theory (like gradient descent) is assumed to be known. Then, a full introduction to reinforcement learning is provided, so to understand PPO.
    \vspace{3mm}
    
    \item \textbf{Chapter 3: Related Works} 
    The approaches of previous works within learning-based motion planning are presented to get an overview of the different methods to solve the task. The motivation for reinforcement learning is put forward in comparison.
    \vspace{3mm}
    
    \item \textbf{Chapter 4: Problem Formulation}
    The learning task of this thesis is presented from a technical perspective. The problem is split into two parts -- a reinforcement and representation learning task.
    \vspace{3mm}
    
    \item \textbf{Chapter 5: Proposed Approach}
    The two-part deep neural network model is proposed. The custom rewards, losses, training setup and architecture of the VAE and MLP are presented and discussed.
    \vspace{3mm}
    
    \item \textbf{Chapter 6: Implementation}
    The method to prepare and train the proposed network is presented, along with various software tools and frameworks. This includes the gathering of data for VAE, the simulation setup and implementation details for optimising our reinforcement learning agent.
    \vspace{3mm}
    
    \item \textbf{Chapter 7: Navigation Policy Evaluation Studies} 
    The step-by-step procedure for training the agent is presented and the results analysed. The agent performance is then evaluated in known environments, when exposed to noise, and in larger environments.
    \vspace{3mm}
    
    \item \textbf{Chapter 8: VAE Evaluation Studies} 
    The results of VAE training with different loss functions are presented and their reconstructions analysed.
    \vspace{3mm}
    
    \item \textbf{Chapter 9: Discussion}
    We explore how a reinforcement learning agent is able to solve a collision task and why it sometimes fails. We discuss the VAE \todo{VAE}
    \vspace{3mm}
    
    \item \textbf{Chapter 10: Conclusions} \vspace{3mm}
    The overall approach, results and discussion points are summarised. We present further work that can be added to this thesis.
    
\end{itemize}





