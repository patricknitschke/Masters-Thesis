\chapter{Related Works}
\label{chap:3_related_works}
\begin{comment}
There is extensive prior work in learning-based methods for autonomous navigation. Together, they encompass a variety of methods, from model-based to model-free, applied in environments including land, air, cluttered and dynamic. 
Despite these differences, the challenge of learning-based autonomous navigation is usually approached through the same three questions:
\begin{enumerate}
    \item How can the environment be perceived? 
    \item What actions can be taken? 
    \item How can these be learned?
\end{enumerate}
\end{comment}

In the complex task of autonomous navigation, learning what to do or which actions to take -- such as the optimal velocity and steering angle for a given input image -- can be challenging to express explicitly. In this chapter, we will explore some related learning-based methods to tackle this problem, covering methods that utilise expert demonstrations in supervised or imitation
learning, or those that rely on a reinforcement learning approach to learn an optimal behaviour.

\section{Motion Planning with Supervised Learning}
\label{sec:3_supervised_learning}
The first learning-based method for navigation is to supervise the learning process by compiling a set of expert demonstrations. The idea here is that we have some deep NN -- typically a CNN -- that will learn to directly predict optimal actions (or action sequences) based on some input -- typically an image from an RGB or depth camera, or laser range findings from a LiDAR sensor. In this approach, the navigation problem is essentially a prediction problem. The main challenge is defining the optimal target action for each input when compiling the dataset of expert demonstrations.

In \cite{pfeiffer2017perception}, they proposed the first target-oriented end-to-end navigation model for a robotic platform, capable of predicting steering commands -- translational and rotational velocities -- from raw 2D-laser range findings and a target position. They use a CNN with residual blocks inspired from \cite{KaimingResNet}, which takes in the LiDAR data and concatenates this feature output with the target position to serve as an input to a fully connected NN. Then, comparing the action prediction from this model to targets from a global planner -- serving as an expert -- allowed \cite{pfeiffer2017perception}  to train their model in an end-to-end fashion. By using a global planner, the authors also avoided requiring a human expert to tediously provide steering commands at a large scale.

In \cite{dronet}, they introduced \textit{DroNet} -- an efficient CNN with a ResNet8 architecture also from \cite{KaimingResNet} -- that can guide a quadrotor in urban environments by predicting steering angles and corresponding collision probabilities from single image inputs. To manage this, \cite{dronet} utilised a dataset created from cars and bicycles to train their CNN instead of compiling their own dataset.
From this, they achieved safe navigation that was highly generalisable and avoided the high sample complexity required for reinforcement learning algorithms.

In autonomous drone racing, one requires a fast perception system capable of real-time detection and pose estimation of gates. This problem can also be approached with a CNN, which then requires the design of a custom dataset due to highly varying race tracks and possible actions. In \cite{supervised_DeepDroneRacing}, they proposed a method of generating a labelled dataset using an expert trajectory and policy. First, an optimal trajectory can be generated through all gates if their poses are known.
With this trajectory, an expert policy can be used to generate a desired direction and speed to follow the trajectory. Finally, by collecting sampling state estimates and corresponding images, one could use the expert policy to label the images to create a labelled dataset. This allowed \cite{supervised_DeepDroneRacing} to train a CNN -- a DroNet from \cite{dronet} -- to learn the desired direction and speed output for each image input. 

\section{Motion Planning with Semi-Supervised Learning}
\label{sec:3_semi_supervised_learning}
Self-supervised learning techniques are closely related to supervised methods, requiring a labelled dataset so to be able to learn desirable actions for inputs. However, this approach does not depend on some expert planner's demonstrations. In contrast, these methods rely solely on a robot's retrospective self-experience to learn the environment's physical attributes.
For example, \cite{learning_to_fly_by_crashing} relies on learning a navigation policy through crashing. They equipped a 720p camera to a quadrotor and tasked it with flying in a straight line until collision. Based on the experience gathered from this simple behaviour, they compiled an image dataset full of positive and negative collision examples and used this to train a CNN to predict whether or not to go straight. By further cropping the images into left and right halves, they created a turning mechanism that allowed the quadrotor to navigate cluttered indoor environments.

The work done in \cite{Badgr} takes this concept further, teaching a mobile ground robot through self-supervision to navigate in ``real-world urban and off-road environments with geometrically distracting obstacles'' with only a camera. The Berkeley autonomous ground robot, \textit{BADGR}, gathers off-policy data in real-world environments from a random control policy, and uses this to train a deep model -- a CNN and Long Short-Term Memory \cite{LSTM} model -- to predict all future navigational events, such as reaching a goal, collisions or driving over bumpy terrain. 
Based on the predicted future events, BADGR then finds an optimal action sequence using a stochastic optimiser \cite{BADGR_stochastic_optimiser} and executes the first action in an MPC-like fashion.

\begin{comment}
    Safe Visual Navigation via Deep Learning and Novelty Detection -- has a collision prediction network, trained from input action pairs -- uses an autoencoder for novelty detection, measures uncertainty in the network -- transitions between high-performance learned model and conservative simple prior.
\end{comment}

Following a similar approach, \cite{deepCollisionPredictorOracle} proposed \textit{ORACLE}, a motion primitives-based navigation planner for a quadrotor using a deep collision predictor. The deep collision predictor is an uncertainty-aware NN model that predicts the collision cost of a predefined set of motion-primitives (action sequences), given some depth image and the only the quadrotor linear and angular velocities.
As for data collection, a quadrotor is deployed in simulation with a random velocity and steering angle  (within the quadrotor field-of-view). Data is collected until collision, and the sequence of state-actions and their collision labels are recorded. This dataset then allows the end-to-end training of the deep collision predictor, where it learns to predict the collision probability of an action for each time step of the action sequence. Further, it makes its predictions uncertainty-aware by filtering depth image observations, taking the unscented transform for the quadrotor partial state, and having Monte Carlo dropout in the CNN. By doing this, \cite{deepCollisionPredictorOracle} also achieved a successful sim-to-real transfer.

\section{Imitation Learning using Expert Planners}
\label{sec:3_imitation_learning}
In the cases where we have direct access to expert planners, we can also apply supervised learning ideas to reinforcement learning to achieve \textit{imitation learning}. This is a sequential task where, given a dataset of demonstrations, an agent tries to find the best way to learn a policy that achieves an action that is most similar to the expert \cite{imitationLearningOverview}. Ideally, this should be very sample efficient and should allow the agent to instantly generalise its policy to new situations of the same task.

In 2013, \cite{monocularReactiveUAVControl} used a novel imitation learning technique with data aggregation, or \textit{DAgger} \cite{DAgger}, to train a reactive heading policy for a quadrotor based on the demonstration of an expert human pilot. In contrast to a supervised technique, their approach iteratively learned and exploited corrective input from a human pilot to boost the overall performance of the predictor. This meant that initially, the agent learns a policy based on the data provided by the expert, but it replays this policy in several training iterations to gather more data, specifically where a human pilot could provide correct steering commands when observing undesired behaviour from the quadrotor, e.g. when it turns towards a tree. 
Using this method, \cite{monocularReactiveUAVControl} managed to learn a policy capable of navigating a quadrotor through cluttered forest environments at $1.5\text{ms}^{-1}$ using only a single, cheap camera.

In 2021, \cite{HighSpeedFlightWild} proposed an end-to-end approach for high-speed flight in natural environments, training solely from simulation. Their student policy is learned via \textit{privileged learning}, whereby imitating an expert with access to privileged information. In this case, the expert is a sampling-based planner with access to the perfect state and 3D map and samples a set of collision-free trajectories conditioned towards some global collision-free trajectory. Then, from this set, the best three trajectories are chosen the student policy.
The result is a CNN that learns how to map simulated noisy depth images directly to collision-free trajectories in a range of real-world environments, including a forest and urban environment where the quadrotor had 100\% success rates for speeds up to $5\text{ms}^{-1}$ and $7\text{ms}^{-1}$ respectively.


\section{Motion Planning with Reinforcement Learning}
\label{sec:3_reinforcement_learning}
The approach most related to this thesis is that of reinforcement learning for motion planning. This approach does not rely on the existence of an expert planner or the need to define target actions for some supervised prediction model. Instead, this is an iterative training process where the reward function is used to define the optimal desired behaviour.

Using reinforcement learning for navigation is not a new concept, with \cite{highSpeedObstacleAvoidanceMonocularVision2005} demonstrating in 2005 a method for high-speed obstacle avoidance using monocular vision and reinforcement learning. In this work, a linear model was first used to learn depth from encoded images, which was then combined with a reinforcement learning algorithm to learn steering commands for an RC car.
Since then, \cite{TowardsMonocularVisionObstacleAvoidanceDeepRL2017} extended this work to deep learning, using a CNN to predict depth from monocular images, and proposed a duelling architecture based deep Q-Network (D3QN) to output command steering and linear velocities for a robot from the depth images. Using this approach, they could train a model entirely in simulation capable of navigating a ground robot in cluttered real-world environments, even with very noise depth predictions.

Another paper that achieves zero-shot transfer from simulation to reality is the work done in \cite{cad2rl}. Here, the authors propose a learning-based algorithm for directly mapping monocular images to collision-free quadrotor motor commands, called \textit{CAD}$^2$\textit{RL}. Here, a reinforcement learning agent chooses one of 41x41 image grid bins to travel to, which is then transformed into a velocity vector. To learn the correct actions, the agent learns a Q-function -- parametrised by a CNN with VGG16 architecture \cite{vgg16} -- via a custom-made policy iteration algorithm, whereby simulating multiple-step rollouts and performing a Monte Carlo policy evaluation.

The methods mentioned above make use of discretised action spaces to simplify the reinforcement learning problem, though as discussed in \cref{subsec:2_an_extension_to_continuous_control}, this does not scale to high-dimensional problems with many degrees of freedom. To amend this, the authors in \cite{virtualToRealRLContinuousControlForMaplessNavigation} introduced a method for continuous control for mapless navigation of mobile robots, using an end-to-end asynchronous deep reinforcement learning approach. They use an asynchronous form of DDPG \cite{DDPG}, where a sparse set of 10 range findings, previous action and target position are mapped to continuous steering commands in the actor network.

However, in a more complex, dynamic environment, having a state representation to represent the world can be beneficial for control tasks \cite{stateRepresentation_overview}. This is especially relevant for problems where an agent's states are only partially observed -- known as partially observed MDPs (POMDPs) -- as not all the information of the environment can be deduced in one observation as not all states hold the Markov property. For these cases, a recurrent neural network (RNN), such as a Long-Short Term Memory (LSTM) \cite{LSTM}, can be used to encode the temporal relationship of observation sequences.

The work done by \cite{MotionPlanningAmongDynamicAgentsDRL2018} leveraged this idea, proposing the use of a Long-Short Term Memory to encode the observations of an arbitrary number of other agents in an environment into a fixed-length vector. To achieve collision avoidance on their ground robot in the presence of other agents, they combined this representation with their robot's own state vector and used this as an input to their actor-critic networks. From this, the agent was able to command steering angles and speeds to be able to navigate amongst humans at walking speed.

Having the same idea for image observations, the authors of \cite{worldModels2018} proposed a network architecture that combines a VAE with a Mixture Density Network (MDN) and RNN (MDN-RNN) to create a representation of OpenAI Gym \cite{openAIgym} environments. Using this architecture, they feed the latent code of the VAE and the RNN hidden state to a very simple linear model that outputs an action. With this, \cite{worldModels2018} managed to solve a range of tasks, among them a race car navigation problem from pixels that had previously not been solved.

Then, in \cite{stochastic_latent_actor_critic}, the authors proposed a principled training procedure for unifying latent representation learning with reinforcement learning. In contrast to end-to-end learning methods, \cite{stochastic_latent_actor_critic} suggests to separate the two tasks: first relying on variational inference to learn a latent representation, then training the reinforcement learning agent using the learned latent space. From this, their experiments show that their algorithm successfully learns complex continuous control tasks from raw images in the OpenAI Gym and DeepMind Control Suite Environments.

Taking inspiration from \cite{worldModels2018}, the authors in \cite{NavRep_unsupervised} also proposed a three-part deep model, but for robotic navigation in dynamic human environments. Their model included a VAE to reconstruct a LiDAR state, an LSTM to predict future state sequences and a 2-layer perceptron taking in the representations of the first two modules as input. Their work focused on testing different variations of this network for the navigation task: altering the LiDAR representation, switching the LSTM to a transformer \cite{transformer}, and training their model jointly in an end-to-end approach or separately as in \cite{worldModels2018}, and \cite{stochastic_latent_actor_critic}. Finally, they demonstrated the performance of one of their models on a real robot, where it reached its goal 100\% of the time, albeit with some room for improvement in its behaviour.

Lastly, the work that has been most inspirational for this thesis the most is that of \cite{LearningStateRepresentation}. Here, the authors learn a state representation from depth images and camera trajectories and use this to train a policy for navigation in cluttered and dynamic environments. They use a VAE to encode depth images and feed the latent vector with camera trajectories to an LSTM to generate a hidden state. Then, a simple MLP acting as the policy receives this state representation and a goal as input, decides on velocities in $x$ and $y$, and a yaw rate. This model achieved only a 3\% failure rate in their tests and was successful in sim-to-real transfer.
A key feature of \cite{LearningStateRepresentation} is that they trained their VAE to perform simple depth completion by comparing reconstructed depth images with a filtered target depth image. By doing so, they minimised the shortcomings of real depth images compared to simulated ones and reduced the complexity of depth images. This idea is also useful when we wish to limit the generative capacity of the VAE to only a small latent dimension.
Finally, other similarities of this work with our thesis include using Isaac Gym as a simulation environment and PPO as the reinforcement learning algorithm of choice. 

