\chapter{Conclusions}
\label{chap:10_conclusions}
\begin{comment}
A concise summary of essential information and findings in the project.
\end{comment}

Learning-based approaches to autonomous navigation in cluttered environments are becoming increasingly popular due to being able to learn flexible, complex behaviours that can solve various challenging tasks without the need to explicitly program them. Due to their ability to represent complex functions through deep architectures, they are able to direct map raw sensor inputs to actions which, combined with the fast parallel computing hardware, allows them to plan and execute actions at a speed at which a model-based pipeline cannot compete.
Further, when presented with a novel task with no prior demonstrations and an optimal solution that is hard to formulate, reinforcement learning provides a method to learn this through only the specification of an abstract reward function.

In this thesis, we thus explored the ability of reinforcement learning to solve a collision avoidance task for a quadrotor using only a depth camera. We proposed the use of a two-part CNN-MLP model, where the CNN is the inference network of the VAE, while the MLP serves as the reinforcement learning agent actor-critic. To shape our model for collision avoidance, we then introduced a novel reward function for the reinforcement learning agent and a novel loss function for the VAE. The reward function was shaped with penalties to demotivate risky behaviour, which in hindsight may not have been optimal due to the inherent sparsity of rewards. Similarly, the customised reconstruction error of the VAE enabled us to prioritise collision-relevant features of depth images, though it potentially resulted in over-fitting of the depth data distribution.

Nonetheless, the evaluation studies showed that the overall approach was successful at its task, achieving 98.6\% in the hard environment which accounted for tight space. The approach also showed promising robustness to noise in the quadrotor state and actions, where its adverse effects were most present in tight paths, such as in the hard environment. Despite these results, the study also showed that much could be improved, both in the proposed approach and the implementation of our model. Specifically, it was shown that many pass-by collisions occur due to blindness when turning, the agent frequently reversed or descended into collisions, and the agent maintained a bias when navigating past obstacles.
It was discussed that to alleviate these problems, the concept of an internal memory had to be added to the policy, through, for example, the hidden state of an RNN, such that the agent is capable of predicting collisions more than one timesteps ahead, remembering passed obstacles, and finally capable of deciding more fine-tuned actions which account for these.
Otherwise, implementation errors included a too small margin between the goal height and ground, and a lack of normalisation in the environment setup. 

Moreover, this thesis demonstrated the importance of a meaningful and well-formed latent space, where we saw that the latent code captured the placements of obstacles and their edges. By adding the filtering operation to be implicitly learned in the forward pass of the VAE, we simplify the complexity of the depth images such that the VAE focuses only on rough shapes, and we achieve a layer of safety when navigating close to obstacles.
However, we saw that the navigation policy is highly dependent on accurate depth images, such that in the future, we should account for noisy depth images by training the VAE to perform denoising.
